{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import rlmep\n",
    "\n",
    "    print(\"Already installed\")\n",
    "except ImportError:\n",
    "    %pip install \"rlmep @ git+https://github.com/Mads-PeterVC/rlmep.git\" # if you are using Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from rich.progress import track\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the exercises at little more accessible some of the code has been implemented \n",
    "as part of the small `rlmep`-package. Some exercises will import small functions or classes from this package. \n",
    "If you're curious about the details of a particular function you can view the source code or have jupyter show it as \n",
    "is done in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlmep import example_function\n",
    "\n",
    "example_function??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 'Frozen Lake'-environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with we will get some familiarity with reinforcement learning by applying Q-learning \n",
    "to a problem that is not directly related to materials science. \n",
    "\n",
    "A convenient library is `gymnasium` which supplies a consistent API for defining \n",
    "environments and supplies a number of toy environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `FrozenLake`-environment our agent is guiding an adventurer across a frozen lake, \n",
    "in certain spots the ice is too thin and the poor adventurer will fall through and die. \n",
    "However, if the adventurer navigates across the ice properly he can reach a gift. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "sz = 4\n",
    "\n",
    "# 0: ←, 1: ↓, 2: →, 3: ↑\n",
    "actions = [None, 1, 2]\n",
    "\n",
    "fig, axs = plt.subplots(1, len(actions), figsize=(len(actions) * sz, 1 * sz))\n",
    "\n",
    "for ax, action in zip(axs, actions):\n",
    "    if action is not None:\n",
    "        _ = env.step(action)\n",
    "\n",
    "    ax.imshow(env.render())\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Try a different sequence of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important aspects of an environment are the action and state spaces.\n",
    "\n",
    "We can inspect these like done below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if an action is part of the space.\n",
    "contains_action = [env.action_space.contains(a) for a in range(5)]\n",
    "print(f\"{env.action_space = }\")\n",
    "print(f\"{contains_action = }\")\n",
    "\n",
    "# Sample an action from the space.\n",
    "sampled_action = env.action_space.sample()\n",
    "print(f\"{sampled_action = }\\n\")\n",
    "\n",
    "# The observation space\n",
    "print(f\"{env.observation_space = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that for each state there are 4 discrete actions and there are 16 states - \n",
    "which matches the figures from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Temporal difference derivation\n",
    "\n",
    "Now that we have an environment, the second thing we need is an algorithm. We will be using Q-learning with a table. \n",
    "\n",
    "Here Q is the action-value function, from the Bellman equation: \n",
    "\n",
    "$$ Q(s_t, a_t) = \\mathbb{E}_{s^*} [r(s_t, a_t) + \\gamma \\mathrm{max}_{a^*} \\ Q(s^*, a^*)] $$\n",
    "\n",
    "The expectation value here is over the possible states resulting from taking action $a$ in state $s$. $\\gamma$ is the discount factor, \n",
    "it weights the importance of future rewards. \n",
    "Given that we have our environment has deterministic transition function we can ignore that expectation value and \n",
    "reach the equation\n",
    "\n",
    "$$ Q(s_t, a_t) = [r(s_t, a_t) + \\gamma \\mathrm{max}_{a^*} \\ Q(s_{t+1}, a^*)] $$\n",
    "\n",
    "So, for a given state-action pair $(s_t, a_t)$ the Q-value is the sum of the reward and the maximum of the next state. \n",
    "Initially we dont know Q, so we will need a learning algorithm. To reach such an algorithm we write an error\n",
    "\n",
    "$$ [r(s_t, a_t) + \\gamma \\mathrm{max}_{a^*} Q(s_{t+1}, a^*)] - Q(s_t, a_t) $$\n",
    "\n",
    "To minimize that, we can update the Q-value according to\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\Big( [r_t + \\gamma \\mathrm{max}_{a^*} Q(s_{t+1}, a^*)] - Q(s_t, a_t) \\Big) \n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is a learning rate parameter, if $\\alpha = 1$ we can see that this update rule reduces to just setting \n",
    "\n",
    "$$ Q(s_t, a_t) \\leftarrow [r(s_t, a_t) + \\gamma \\mathrm{max}_{a^*} \\ Q(s_{t+1}, a^*)] $$\n",
    "\n",
    "Which is essentially taking the Bellman equation and changing the equals sign to an arrow. \n",
    "This is the **temporal difference** target. \n",
    "\n",
    "At each step you will need to calculate \n",
    "\n",
    "$$\n",
    "\\Delta Q(s_t, a_t) = \\Big( [r_t + \\gamma \\mathrm{max}_{a^*} Q(s_{t+1}, a^*)] - Q(s_t, a_t) \\Big) \n",
    "$$\n",
    "\n",
    "And then update the Q-table for that action\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\mathrel{{+}{=}}  \\alpha \\Delta Q(s_t, a_t)\n",
    "$$\n",
    "\n",
    "\n",
    "### $\\epsilon$-greedy policy\n",
    "\n",
    "Reinforcement learning often happens in 'episodes' of letting the 'agent'/algorithm explore the environment, gathering \n",
    "information and possibly training. Our agent in this case is defined by the Q-table, generally in RL we say that the agent follows a 'policy', in this case our policy will mostly be to choose actions according to $\\mathrm{argmax}(Q(s, a))$. In order to have some exploration of the environment we will modify this greedy strategy, leading us to a so-called $\\epsilon$-greedy policy. \n",
    "\n",
    "It goes as follows: \n",
    "- Draw some random number $z$\n",
    "- If $z < \\epsilon$ draw a random action. \n",
    "- Else choose according to $\\mathrm{argmax} \\ Q(s, s)$\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "Now we have all the elements required for our algorithm, so lets put it together. \n",
    "\n",
    "Our algorithm will thus be as follows: \n",
    "\n",
    "- Create a Q-table of just zeros of size (16, 4)\n",
    "\n",
    "1. Choose an action according to the $\\epsilon$-greedy strategy. \n",
    "2. Take a step in the environment with that action, resulting in the next state, reward and a signal of whether the state is terminal or not. \n",
    "3. Update the Q-value according to the temporal difference rule from above. \n",
    "4. If the action lead to a terminal state end the episode. \n",
    "\n",
    "### Exercise: \n",
    "\n",
    "Finish the tabular Q-learning algorithm below. \n",
    "\n",
    "The cell after the cell below will plot the Q-values, so you can test your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\", is_slippery=False)\n",
    "\n",
    "Q_table = np.zeros((16, 4), dtype=np.float64)\n",
    "\n",
    "# Q_table = np.random.uniform(low=-1, high=1, size=(16, 4))\n",
    "\n",
    "# Hyperparameters\n",
    "epsilon = 0.5\n",
    "learning_rate = 1\n",
    "gamma = 0.9\n",
    "num_episodes = 250\n",
    "\n",
    "\n",
    "for episode in track(range(num_episodes)):\n",
    "    state, _ = env.reset()\n",
    "    terminal = False\n",
    "    truncated = False\n",
    "\n",
    "    while not terminal and not truncated:\n",
    "        # Choose action\n",
    "        # If random number is less than epsilon, then select a random action\n",
    "        # Else select according to the Q-values. When selecting according to Q-values,\n",
    "        # if all actions have the same Q-value, then select a random action.\n",
    "        # This is done as argmax will always select the first index in the case of a tie.\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = ... # Your code here\n",
    "        else:\n",
    "            if (Q_table[state, :] == Q_table[state, 0]).all(): # If all actions have the same Q-value\n",
    "                action = ... # Your code here\n",
    "            else:\n",
    "                action = ... # Your code here\n",
    "\n",
    "        # Take action by calling env.step\n",
    "        next_state, reward, terminal, truncated, info = env.step(action)\n",
    "\n",
    "        # Calculate Q-target\n",
    "        Q_target = ...\n",
    "\n",
    "        # Update Q-table\n",
    "        Q_table[state, action] += ...\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to figure out whether things are working, the following cell will plot the Q-values learned by your agent. \n",
    "\n",
    "The figure shows the learned Q-values for each action of each state, if any of the Q-values differ from zero. \n",
    "To understand from the visualization whether your implementation is working or not, you should consider the \n",
    "following questions;\n",
    "\n",
    "- What should the Q-value be for state-action pairs that lead to the goal be? \n",
    "- What should Q-values for state-actions pairs that lead to hole tiles be? \n",
    "- How should the Q-values depend on the discount factor $\\gamma$? \n",
    "- What should the Q-values for illegal actions be? (E.g. actions that attempt to move the adventurer out of the grid.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlmep.exercise_1 import plot_qvalues\n",
    "\n",
    "plot_qvalues(Q_table, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "As a final task for this tabular Q-learning algorithm try to alter your code such that it works with an initial Q-table that is not just zeros. This will be relevant for an upcoming topic, Q-learning with neural networks where we cannot easily ensure that initial Q-values are all zero. \n",
    "\n",
    "At first just change such that the initial Q-table is initialized randomly; \n",
    "\n",
    "```python\n",
    "Q_table = np.random.uniform(low=-1, high=1, size=(16, 4))\n",
    "```\n",
    "and rerun the training of the algorithm.  What happens? Can you fix it? \n",
    "\n",
    "<br> \n",
    "\n",
    "<details>\n",
    "<summary> <strong> Hint </strong> </summary>\n",
    "\n",
    "Unless you had a lot of foresight while implementing your training loop this likely doesn't work. \n",
    "The issue stems from the update rule\n",
    "\n",
    "$$ \\Delta Q(s_t, a_t) = \\Big( r(s_t, a_t) + \\gamma \\mathrm{max}_{a^*} \\ Q(s_{t+1}, a^*) - Q(s_t, a_t) \\Big) $$\n",
    "\n",
    "Which is only valid for actions that lead to **non-terminal** states, otherwise we should \n",
    "apply the rule\n",
    "\n",
    "$$ \\Delta Q(s_t, a_t) = \\Big( r(s_t, a_t) - Q(s_t, a_t) \\Big) \\ \\mathrm{if} \\ s_{t+1} \\ \\mathrm{is \\ terminal}$$\n",
    "\n",
    "To fix this in the training loop you can just have an `if`-statement when calculating the `Q_target`:\n",
    "\n",
    "```python\n",
    "if not terminal:\n",
    "    Q_target = ...\n",
    "else:\n",
    "    Q_target = ...\n",
    "```\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-network rather than Q-table. \n",
    "\n",
    "Having a table in 2025 is very embarrassing, so let's replace that table with a network. \n",
    "\n",
    "What do we want from that network? We want it to be able to predict action-values $Q$ \\\n",
    "given a state and an action, but that is not completely specific. \n",
    "\n",
    "We have multiple options: \n",
    "- The network could take both a state $s$ and an action $a$ and predict a single Q-value; \n",
    "- The network could take only the state and predict all Q-values for that particular state. \n",
    "\n",
    "We will opt for the second option, but the first one should work as well. \n",
    "\n",
    "So for the first option we want a network that \n",
    "- Takes a state representation as an input; \n",
    "- Outputs 4 numbers corresponding to the 4 actions available for each state. \n",
    "\n",
    "Finish the implementation of the `Qnet`-class below, this involves; \n",
    "\n",
    "- `build_network`: Should create a neural network with the right input and output shapes.\n",
    "- `train`: Should perform a single training step.\n",
    "\n",
    "<details>\n",
    "<summary> <strong> Hint: Neural networks </strong> </summary>\n",
    "\n",
    "Your network should consists of `torch.nn.Linear`-layers with some activation function, e.g. `torch.nn.ReLU`. \n",
    "\n",
    "Initialization of `torch.nn.Linear` takes the input dimension and the output dimension. \n",
    "\n",
    "To create a stack of layers you can use `torch.nn.Sequential`.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br> \n",
    "\n",
    "<details>\n",
    "<summary> <strong> Hint: Training step </strong> </summary>\n",
    "\n",
    "You can use `torch.nn.functional.mse_loss` or you can just write it out as an element-wise array operation - like numpy. \n",
    "\n",
    "To calculate gradient just put `loss.backward()`. \n",
    "\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class Qnet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=1e-3,\n",
    "        hidden_dim=16,\n",
    "        input_dim=1,\n",
    "        output_dim=4,\n",
    "        network_kwargs: dict | None = None,\n",
    "        state_transform: Callable | None = None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        if network_kwargs is None:\n",
    "            network_kwargs = {}\n",
    "\n",
    "        if state_transform is None:\n",
    "            state_transform = lambda x: x  # noqa: E731\n",
    "\n",
    "        self.network = self.build_network(\n",
    "            input_dim, hidden_dim, output_dim, **network_kwargs\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.state_transform = state_transform\n",
    "\n",
    "    def build_network(self, input_dim: int, hidden_dim: int, output_dim: int, **kwargs):\n",
    "        network = ... # Your code here\n",
    "\n",
    "        return network\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        This method takes an input tensor `x`, applies the state transformation,\n",
    "        and computes the Q-values for each action.\n",
    "        \"\"\"\n",
    "        x = self.state_transform(x)\n",
    "        q = self.network(x).view(-1, self.output_dim)\n",
    "        return q\n",
    "\n",
    "    def train(self, states: torch.Tensor, actions: torch.Tensor, targets: torch.Tensor):\n",
    "        # Forward pass\n",
    "        Q_values = ... # Your code here\n",
    "\n",
    "        # This selects Q corresponding to the actions taken\n",
    "        Q_action = Q_values[range(Q_values.shape[0]), actions]\n",
    "\n",
    "        # Compute loss: MSE between Q_action and targets\n",
    "        loss = ... # Your code here\n",
    "\n",
    "        # Calculate gradients\n",
    "        ... # Your code here\n",
    "\n",
    "        # Take step and reset gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return (\n",
    "            loss.detach().numpy()\n",
    "        )  # Returning just for book-keeping, not necessary for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell you can check whether your network's forward pass works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlmep.exercise_1 import Qnet\n",
    "\n",
    "# Check that our network works as expected.\n",
    "model = Qnet(input_dim=1, output_dim=4)\n",
    "s = torch.tensor([0], dtype=torch.float32)\n",
    "Q = model.forward(s)\n",
    "\n",
    "print(Q.shape)\n",
    "print(Q)\n",
    "assert Q.shape == (1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to test that our network is working and capable of expressing the Q-table we can try \n",
    "to just train it towards table we created before.\n",
    "\n",
    "I provided the function `state_transform` that turns the integer representation of \n",
    "the state into a grid coordinate representation with two integers. \n",
    "\n",
    "Try training with and without the `state_transform` - remember that this changes the input dimension of \n",
    "the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlmep.exercise_1 import get_batch\n",
    "\n",
    "\n",
    "def state_transform(state):\n",
    "    i = state // 4\n",
    "    j = state % 4\n",
    "    return torch.hstack([i, j])\n",
    "\n",
    "\n",
    "# Train the network\n",
    "model = Qnet(learning_rate=1e-3, input_dim=2, state_transform=state_transform)\n",
    "loss = []\n",
    "for _ in track(range(5000), description=\"Training Q-network\"):\n",
    "    states, actions, targets = get_batch(Q_table, size=16)\n",
    "    l = ...  # Your code here\n",
    "    loss.append(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate all the predictions and plot against the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a table of its predictions:\n",
    "Q_table_nn = np.zeros((16, 4), dtype=np.float64)\n",
    "\n",
    "for state in range(16):\n",
    "    state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "    Q_table_nn[state] = model.forward(state_tensor).detach().numpy()\n",
    "\n",
    "sz = 4.5\n",
    "fig, axs = plt.subplots(1, 2, figsize=(2 * sz, 1 * sz))\n",
    "\n",
    "plot_qvalues(Q_table, env=env, ax=axs[0], fs=11)\n",
    "plot_qvalues(Q_table_nn, env=env, ax=axs[1], fs=11)\n",
    "\n",
    "axs[0].set_title(\"Q-table\")\n",
    "axs[1].set_title(\"Q-network\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network: Q-learning with neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running an episode for the training of the Q-table we updated the table every step, \n",
    "whereas when we trained the network towards the table we used batches of data. \n",
    "\n",
    "Using batches is both more efficient in terms of computational speed, but also provides \n",
    "more reliable gradients - so we would like to use batches of data for our Q-network learning \n",
    "algorithm. To do so we will create an **experience replay memory**, that we can draw batches from, and we \n",
    "will not be training after every action. \n",
    "\n",
    "Our Q-learning algorithm with a table was an *on-policy* algorithm, we were updating the \n",
    "table with rewards and Q-values found with the current policy, as we were simply just\n",
    "updating according to the most recent action. With the experience replay, we get an *off-policy* algorithm, \n",
    "the training data does not necessarily come from the current policy. \n",
    "\n",
    "The temporal difference target that we used before, depends on the predictions of the network for new state: \n",
    "\n",
    "$TD = [r(s_t, a_t) + \\gamma \\mathrm{max}_{a^*} \\ Q(s_{t+1}, a^*)]$\n",
    "\n",
    "That can create some instability, so a trick/innovation to stabilize the training is to use \n",
    "two networks. \n",
    "- The main network that we are training\n",
    "- A target network where the weights are copied from the main network every so often. \n",
    "\n",
    "This avoids training the main network towards a constantly moving target. \n",
    "\n",
    "So an outline of the algorithm we want is as follows: \n",
    "\n",
    "0. Create two networks: main and target. \n",
    "1. Run a number of steps to gather data that we store in an experience replay buffer. \n",
    "2. Train main network on data from the experience replay and the predictions of the **target** network.\n",
    "3. Occasionally copy the weights from the main network to the target network. \n",
    "\n",
    "Or in pseudocode\n",
    "\n",
    "<img src=\"images/deep_q_learning.png\" />\n",
    "\n",
    "From [Mnih et al., Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236/)\n",
    "\n",
    "The notation in the pseudocode example is slightly different because they convert\n",
    "states $s$ to $\\phi$, as they deal with frames of a game where multiple frames\n",
    "are stacked together to give a final representation of a state - this allows\n",
    "representation of moving objects. But for us this is not important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement a `DQN`-class that implements this algorithm. \n",
    "\n",
    "As noted we will need a replay buffer that holds the experiences of our agents as it \n",
    "traverses the environment. This is mostly uninteresting bookkeeping, so I provide a `ExperienceReplay`-class that \n",
    "handles this.\n",
    "\n",
    "Your job is to finish the implementation of the three methods; \n",
    "\n",
    "- `get_action`: Implement the $\\epsilon$-greedy policy to get an action.\n",
    "\n",
    "- `rollout`: Do one 'play-through' of the environment - e.g. step through until the agent reaches a terminal state or is \n",
    "truncated. \n",
    "\n",
    "- `train_network`: Train the `main_network` on a batch of data from the replay buffer.\n",
    "\n",
    "<br> \n",
    "\n",
    "<details>\n",
    "<summary> <strong> Hint: `get_action` </strong> </summary>\n",
    "\n",
    "Consider if you can use the environment to pick an action - what can be done with the `action_space`-object? \n",
    "\n",
    "You can use `torch.argmax` and the `.item()`-method on a tensor to get an integer.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br> \n",
    "\n",
    "<details>\n",
    "<summary> <strong> Hint: `rollout` </strong> </summary>\n",
    "\n",
    "Use your `get_action`-method to get an action \n",
    "\n",
    "Call `env.step` with your action\n",
    "\n",
    "Use the `train_network` method to do a training a step.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br> \n",
    "\n",
    "<details>\n",
    "<summary> <strong> Hint: `train_network` </strong> </summary>\n",
    "\n",
    "Remember to use the `target_network`! \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlmep.exercise_1 import ExperienceReplay\n",
    "from copy import deepcopy\n",
    "\n",
    "class DQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        main_network: Qnet,\n",
    "        target_network: Qnet,\n",
    "        replay: ExperienceReplay,\n",
    "        gamma: float = 0.90,\n",
    "        epsilon: Callable = lambda x: 0.1,\n",
    "        train_interval: int = 10,\n",
    "        copy_interval: int = 100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the DQN learner with the environment, main network, target network, and replay buffer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        main_network : torch.nn.Module\n",
    "            The main neural network used for action-value function approximation.\n",
    "        target_network : torch.nn.Module\n",
    "            The target neural network used for stabilizing training.\n",
    "        replay : ExperienceReplay\n",
    "            The experience replay buffer used to store and sample experiences.\n",
    "        \"\"\"\n",
    "        self.main_network = main_network\n",
    "        self.target_network = target_network\n",
    "        self.copy_weights()\n",
    "        self.replay = replay\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.train_interval = train_interval\n",
    "        self.copy_interval = copy_interval\n",
    "\n",
    "    def get_action(self, env: gym.Env, state: torch.Tensor, episode: int, apply_epsilon: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        Chooses an action based on the current state and the epsilon-greedy policy.\n",
    "\n",
    "        If a random number is less than epsilon, a random action is selected.\n",
    "        Otherwise, the action with the highest Q-value is selected.\n",
    "        \"\"\"\n",
    "\n",
    "        if np.random.rand() < self.epsilon(episode) and apply_epsilon:\n",
    "            action = ... # Your code here\n",
    "        else:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                Q = ... # Your code here\n",
    "                action = ... # Your code here\n",
    "        return action\n",
    "\n",
    "    def rollout(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        episode: int,\n",
    "        train: bool = True,\n",
    "        apply_epsilon: bool = True,\n",
    "    ):\n",
    "        state, _ = env.reset()\n",
    "        states = [state]\n",
    "\n",
    "        terminal = False\n",
    "        truncated = False\n",
    "        return_value = 0\n",
    "\n",
    "\n",
    "        while not terminal and not truncated:\n",
    "            # Choose action\n",
    "            action = ... # Your code here\n",
    "\n",
    "            # Take action by calling env.step\n",
    "            # Note: env.step returns next_state, reward, terminal, truncated, info\n",
    "\n",
    "            ... # Your code here\n",
    "\n",
    "            return_value += reward\n",
    "\n",
    "            if train:\n",
    "                # Add to replay buffer\n",
    "                self.replay.add(state, action, reward, next_state, terminal)\n",
    "\n",
    "                # Train the network.\n",
    "                if self.step_count % self.train_interval == 0 and self.step_count > 200:\n",
    "                    # Call the train_network method to update the main network\n",
    "                    ... # Your code here\n",
    "\n",
    "                # Copy weights from the main network to the target network\n",
    "                if self.step_count % self.copy_interval == 0:\n",
    "                    self.copy_weights()\n",
    "\n",
    "                self.step_count += 1\n",
    "\n",
    "            # Update the state.\n",
    "            state = next_state\n",
    "            states.append(state.copy())\n",
    "\n",
    "        return return_value, states\n",
    "\n",
    "    def train_network(self, gamma: float):\n",
    "        # Get a batch of data from the replay buffer\n",
    "        states, actions, rewards, new_states, terminal = self.replay.get_batch()\n",
    "        batch_size = len(states)\n",
    "\n",
    "        # Calculate the Q-values for the current state\n",
    "        Q = ... # Your code here\n",
    "\n",
    "        # Calculate the Q-values for the next state - consider which network to use here.\n",
    "        with torch.no_grad():  # Do not track gradients for the target network\n",
    "            Q_next = ... # Your code here\n",
    "            Q_next_max = ... # Your code here\n",
    "\n",
    "        # Calculate the TD-target - Remember to use the terminal states such that\n",
    "        # the target for terminal states is just the reward.\n",
    "        # torch.logical_not is useful here.\n",
    "        td_target = ...\n",
    "\n",
    "        # Calculate the loss\n",
    "        Q = Q[torch.arange(batch_size), actions]\n",
    "        loss = ... # Your code here\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        self.main_network.optimizer.step()\n",
    "        self.main_network.optimizer.zero_grad()\n",
    "\n",
    "    def learn(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        num_episodes=1000,\n",
    "    ):\n",
    "        # This wraps our environment to record statistics about the episodes.\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=num_episodes)\n",
    "\n",
    "        # Reset the replay buffer\n",
    "        self.replay.clear()\n",
    "        self.step_count = 0\n",
    "\n",
    "        for episode in track(\n",
    "            range(num_episodes), description=\"Training DQN\", transient=True\n",
    "        ):\n",
    "            self.rollout(env=env, episode=episode, train=True)\n",
    "\n",
    "        return env.return_queue, env.length_queue\n",
    "\n",
    "    def test(self, env: gym.Env, num_episodes=100, apply_epsilon=False):\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=num_episodes)\n",
    "\n",
    "        for episode in track(\n",
    "            range(num_episodes), description=\"Testing DQN\", transient=True\n",
    "        ):\n",
    "            self.rollout(\n",
    "                env=env, episode=episode, train=False, apply_epsilon=apply_epsilon\n",
    "            )\n",
    "\n",
    "        return np.array(env.return_queue).flatten(), np.array(\n",
    "            env.length_queue\n",
    "        ).flatten()\n",
    "\n",
    "    def copy_weights(self):\n",
    "        self.target_network.load_state_dict(deepcopy(self.main_network.state_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of whether things are working you can run the `learn` method as\n",
    "set up in the cell below, and visualize the Q values with the subsequent cell. \n",
    "\n",
    "Once your code can at least run - change the number of episodes to 1000 and see if your \n",
    "agent can solve the problem - which it can if everything is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "\n",
    "# Set these according to the environment\n",
    "input_dim = 2\n",
    "n_actions = 4\n",
    "\n",
    "# Choose network settings:\n",
    "main_network = Qnet(input_dim=input_dim, state_transform=state_transform)\n",
    "target_network = Qnet(input_dim=input_dim, state_transform=state_transform)\n",
    "\n",
    "# Setup a experience replay.\n",
    "replay = ExperienceReplay(1, size=1000, batch_size=64)\n",
    "\n",
    "# Make a learner & train it\n",
    "learner = DQN(\n",
    "    main_network=main_network,\n",
    "    target_network=target_network,\n",
    "    replay=replay,\n",
    "    gamma=0.90,\n",
    "    train_interval=1,\n",
    "    copy_interval=100,\n",
    "    epsilon=lambda i: 0.1,\n",
    ")\n",
    "num_episodes = 1000\n",
    "\n",
    "returns, lengths = learner.learn(env=env,\n",
    "    num_episodes=num_episodes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a table of its predictions:\n",
    "Q_table_nn = np.zeros((16, 4), dtype=np.float64)\n",
    "\n",
    "for state in range(16):\n",
    "    state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "    Q_table_nn[state] = main_network.forward(state_tensor).detach().numpy()\n",
    "\n",
    "plot_qvalues(Q_table_nn, env=env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is rather sensitive to many of the parameters. So if things are not working but you're convinced the code is correct, try varying:\n",
    "\n",
    "- the epsilon strategy; \n",
    "- the number of episodes; \n",
    "- how often the target network is updated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other environments. \n",
    "\n",
    "<img src=\"images/cart_pole.gif\" />\n",
    "\n",
    "Now that we have a somewhat general DQN implementation, let's try it on an entirely different environment.\n",
    "\n",
    "A consequence of using a neural network rather than a table that we have not \n",
    "discussed so far, is that we are able to handle environments with **continuous** \n",
    "observation/state spaces. \n",
    "\n",
    "An environment with a continuous observation space from `gymnasium` is [Cart Pole](https://gymnasium.farama.org/environments/classic_control/cart_pole/). This is a materials science problem where a materially wooden pole has to be balanced by the movement of a small material cart.\n",
    "\n",
    "Look at the documentation, or play around with `env.observation_space` and `env.action_space` to figure out how this environment works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the cartpole environment.\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Set the action and observation dimensions:\n",
    "obs_dim = 4\n",
    "n_actions = 2\n",
    "\n",
    "# Set the network settings:\n",
    "main_network = Qnet(input_dim=obs_dim, output_dim=n_actions)\n",
    "target_network = Qnet(input_dim=obs_dim, output_dim=n_actions)\n",
    "replay = ExperienceReplay(observation_dim=obs_dim, size=1000, batch_size=32)\n",
    "\n",
    "# Make a learner:\n",
    "learner = DQN(\n",
    "    main_network=main_network,\n",
    "    target_network=target_network,\n",
    "    replay=replay,\n",
    "    gamma=0.99,\n",
    "    train_interval=1,\n",
    "    copy_interval=100,\n",
    "    epsilon=lambda i: 0.1\n",
    "    )\n",
    "\n",
    "num_episodes = 500\n",
    "returns, lengths = learner.learn(env=env, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_returns, test_lengths = learner.test(env=env, num_episodes=100, apply_epsilon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_average(x, N):\n",
    "    return np.convolve(np.array(x).flatten(), np.ones((N,)) / N, mode=\"same\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "ax[0].plot(returns, label=\"Returns\")\n",
    "ax[0].plot(rolling_average(returns, 50), label=\"Returns (rolling average)\")\n",
    "ax[0].set_title(\"Training Returns\")\n",
    "ax[0].set_xlabel(\"Episode\")\n",
    "ax[0].set_ylabel(\"Return\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(test_returns, label=\"Returns\")\n",
    "ax[1].set_title(\"Test Returns\")\n",
    "ax[1].set_xlabel(\"Episode\")\n",
    "ax[1].set_ylabel(\"Return\")\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum return is 500, meaning that the agent balances the pole for 500 time steps, \n",
    "your agent should be able to do this quite consistently during testing.\n",
    "\n",
    "The same implementation of our algorithm can solve problems that are completely different!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
