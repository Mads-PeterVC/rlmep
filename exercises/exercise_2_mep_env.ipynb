{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import rlmep\n",
    "\n",
    "    print(\"Already installed\")\n",
    "except ImportError:\n",
    "    %pip install \"rlmep @ git+https://github.com/Mads-PeterVC/rlmep.git\" # if you are using Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rlmep.exercise_2 import example_figure\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Materials Science RL Environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've developed an RL algorithm it would be cool to apply it to a material's science problem. \n",
    "\n",
    "So this exercise will involve the design and implementation of such an environment. \n",
    "Specifically, we will try to frame **minimum energy path** (MEP) finding as an \n",
    "RL environment. \n",
    "\n",
    "This involves the following decisions\n",
    "\n",
    "- What is the state space?\n",
    "- What is the action space? \n",
    "- What is the reward structure? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Deep Q-learning algorithm can handle continuous state spaces, but only discrete action spaces.\n",
    "\n",
    "So for this reason we will start with a discrete state and action space. To keep things simple we will limit \n",
    "the scope a little (perhaps more than that) and only consider the movement of a single atom. \n",
    "\n",
    "I've made an illustration of the kind of environment we will create below.\n",
    "Here the red dot indicates the starting position and the initial position and the \n",
    "green the final position of an atom. Rather than avoiding drowning, we want our RL algorithm \n",
    "to learn to avoid taking a path that crosses high-energy regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement MEP-environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state-space of our MEP problem will be a 2d-grid. To make handling this grid \n",
    "a little easier the `GridSpec`-class provides some methods; \n",
    "\n",
    "- `__init__`: Taking the arguments\n",
    "    - `grid_size` e.g. `(10, 10)`\n",
    "    - `grid_spacing` e.g. `0.5`\n",
    "    - `corner` e.g. `(0, 0)`\n",
    "    - `height` e.g `10.0`.\n",
    "\n",
    "- `ij_to_xyz`: Takes two arguments `i` and `j` and returns `x`, `y`, `z`.\n",
    "- `xy_to_ij`: Takes two arguments `x` and `y` and returns `i` and `j`.\n",
    "- `visualize`: Takes an `Axes`-object and plots the grid.\n",
    "\n",
    "Convince yourself that these methods work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlmep.exercise_2 import GridSpec\n",
    "\n",
    "grid_spec = GridSpec(grid_size=(10, 10), grid_spacing=0.5, corner=(0, 0))\n",
    "\n",
    "grid_spec.ij_to_xyz(1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our environment we will call the position of the moving atom on the grid our `state` which are the \n",
    "two indices `(i, j)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our environment also needs to be able to determine when it has reached a terminal or truncated state. \n",
    "\n",
    "We will consider two types of terminal states; \n",
    "\n",
    "- If the atom has reached the desired final position that is a terminal state. \n",
    "- If the atom has reached a state that has an energy higher than a particular threshold above the energy of the initial state.\n",
    "\n",
    "In the cell below you should implement these two conditions, the function takes four \n",
    "inputs \n",
    "\n",
    "- `state`: The current state.\n",
    "- `terminal_state`: The state of the final configuration.\n",
    "- `history`: The history of the energy of all states visited.\n",
    "- `delta_max`: The threshold above which to terminate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_terminal(state: list[int, int], terminal_state: list[int, int], history: list[float], delta_max: float) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the state is terminal.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    terminal = ...\n",
    "\n",
    "    return terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the agent going on infinitely looping walks we will truncate an episode if a \n",
    "maximum amount of steps have been taken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_truncated(step: int, max_steps: int) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the episode should be truncated.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    truncated = ...\n",
    "    return truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space is again a discrete space with four possible values. \n",
    "\n",
    "We are mapping (0, 1, 2, 3) to each direction (right, left, up, down) - to conform with the plotting methods \n",
    "we want; \n",
    "\n",
    "- $ 0 \\rightarrow \\mathrm{right}$\n",
    "- $ 1 \\rightarrow \\mathrm{left}$\n",
    "- $ 2 \\rightarrow \\mathrm{up}$\n",
    "- $ 3 \\rightarrow \\mathrm{down}$\n",
    "\n",
    "Complete the function below such when provided an integer it provides the two integers corresponding to a direction on the grid - e.g. \n",
    "\n",
    "$ 0 \\rightarrow (1, 0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_to_direction(action: int) -> list[int, int]:\n",
    "    # Your code here\n",
    "    direction = ...\n",
    "    return direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to move the atom on the grid we also want to make sure it doesn't move out of bounds. \n",
    "\n",
    "So we have to make sure that a move cannot take result in grid coordinates below zero - or grid coordinates above the grid size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state(\n",
    "    state: list[int, int], action: int, grid_size: tuple[int, int]\n",
    ") -> list[int, int]:\n",
    "    direction = action_to_direction(action)\n",
    "\n",
    "    # Your code here\n",
    "    new_state = ...\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to our integers to represent the state we would also like to manipulate an `ase.Atoms`-object \n",
    "based on the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import Atoms\n",
    "\n",
    "\n",
    "def update_atoms(\n",
    "    atoms: Atoms, state: list[int, int], grid_spec: GridSpec, move_index: int\n",
    ") -> Atoms:\n",
    "    \"\"\"\n",
    "    Update the `ase.Atoms` object based on the state and move index.\n",
    "\n",
    "    Use the `grid_spec`-object to convert the state to coordinates.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    atoms = ...\n",
    "\n",
    "    return atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward function determines what 'solving' the environment looks like - we need to \n",
    "encode our intent into the reward function. \n",
    "\n",
    "One choice is to say we record the energy difference at each state visited and \n",
    "once the final state is reached we base the reward on this trajectory. \n",
    "\n",
    "If we define \n",
    "\n",
    "$$\n",
    "\\Delta E_i = E_i - E_{\\mathrm{init}}\n",
    "$$\n",
    "\n",
    "And we introduce two parameters $\\Delta_{max}$ and $A$ we can write a reward for reaching \n",
    "the final state \n",
    "\n",
    "$$\n",
    "r = A \\left(1 - \\frac{\\min \\ [\\max_i \\ ( \\Delta E_i), \\Delta_{max}]}{\\Delta_{max}}\\right)\n",
    "$$\n",
    "\n",
    "Additionally, we can do a bit of reward engineering by at each step rewarding the agent \n",
    "if it moves closer to the final state and punishing it if it moves away. One choice \n",
    "is \n",
    "\n",
    "$$\n",
    "r = \n",
    "\\begin{cases}\n",
    "    r_d, & \\text{if } d_i < d_{i-1} \\\\\n",
    "    -r_d, & \\text{if } d_i > d_{i-1}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Below `current_distance` is $d_i$, `previous_distance` is $d_{i-1}$ and `distance_parameter` is $r_d$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_reward(delta_energies: list[float], A: float, delta_max: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the final reward based on the maximum change in energy.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    reward = None\n",
    "    return reward\n",
    "\n",
    "def get_distance_reward(current_distance: float, previous_distance: float, distance_parameter: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the reward based on the distance to the target.    \n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    reward = None\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "\n",
    "I've created a class that takes your function definitions from above and puts it together \n",
    "with a few tedious tidbits done for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlmep.exercise_2.discrete_mep_scaffold import (\n",
    "    ScaffoldDiscreteMEP,\n",
    ")\n",
    "from rlmep.exercise_2.diffusion_env import get_diffusion_state_grid\n",
    "from ase.calculators.emt import EMT\n",
    "\n",
    "functions = {\n",
    "    \"check_terminal\": check_terminal,\n",
    "    \"check_truncated\": check_truncated,\n",
    "    \"update_state\": update_state,\n",
    "    \"update_atoms\": update_atoms,\n",
    "    \"get_final_reward\": get_final_reward,\n",
    "    \"get_distance_reward\": get_distance_reward,\n",
    "}\n",
    "\n",
    "intial_config, final_config, grid_spec = get_diffusion_state_grid(\n",
    "    grid_size=(12, 7),\n",
    "    grid_spacing=0.5,\n",
    "    grid_shift=(-3, -3),\n",
    ")\n",
    "\n",
    "# You can try this one if you want a finer grid.\n",
    "# intial_config, final_config, grid_spec = get_diffusion_state_grid(\n",
    "#     grid_size=(24, 14),\n",
    "#     grid_spacing=0.25,\n",
    "#     grid_shift=(-6, -6),\n",
    "# )\n",
    "\n",
    "env = ScaffoldDiscreteMEP(\n",
    "    initial_config=intial_config,\n",
    "    final_config=final_config,\n",
    "    gridspec=grid_spec,\n",
    "    functions=functions,\n",
    "    moving_atom=-1,\n",
    "    max_steps=100,\n",
    "    barrier_max=2.5,\n",
    "    reward_scale=10.0,\n",
    "    distance_parameter=0.1,\n",
    "    calculator=EMT(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having assembled the environment we should do a bit of sanity checking that it works as expected. \n",
    "\n",
    "In the cell below you can play around with different ways of stepping through the \n",
    "environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_trajectory = []  # Use to make a specific action trajectory\n",
    "state, info = env.reset()\n",
    "states = [state]\n",
    "\n",
    "return_value = 0.0\n",
    "\n",
    "for step in range(50):\n",
    "    # action = env.action_space.sample()  # Random action\n",
    "    action = 0  # Always move right\n",
    "    # action = action_trajectory.pop(0) if action_trajectory else env.action_space.sample()  # Use predefined actions or random\n",
    "\n",
    "    state, reward, terminal, truncated, info = env.step(action)\n",
    "    return_value += reward\n",
    "    states.append(state)\n",
    "\n",
    "    if terminal or truncated:\n",
    "        break\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "env.reset()\n",
    "env.visualize(ax=ax[0], dx=0.5, dy=0.5, plot_moving=False, state_history=states)\n",
    "\n",
    "\n",
    "ax[1].plot(info[\"history\"], \"-o\", label=\"Energy\")\n",
    "ax[1].set_xlabel(\"Step\")\n",
    "ax[1].set_ylabel(\"Energy\")\n",
    "ax[1].set_title(\n",
    "    f\"Return: {return_value:0.2f} \\n $\\max(\\Delta E) = ${np.max(info['history']):0.2f}\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to check: \n",
    "\n",
    "- Set the `reward_scale` to 0.0 to judge if the distance reward works correctly. \n",
    "- Does the entire terminate correctly? E.g. terminates if the final state is reached or if a too high energy state is visisted.\n",
    "- Do the actions do the intended thing? Is the agent able to move in all directions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below import my versions of `Qnet` and of `DQN` - if you want you can also copy your own implementations\n",
    "from the previous exervise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlmep.exercise_1.dqn_learner import DQN\n",
    "from rlmep.exercise_1.qnet import Qnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlmep.exercise_1.experience_replay import ExperienceReplay\n",
    "import torch\n",
    "\n",
    "env.reset()\n",
    "\n",
    "# Set the action and observation dimensions:\n",
    "obs_dim = 2\n",
    "n_actions = 4\n",
    "\n",
    "# Set the network settings:\n",
    "main_network = Qnet(input_dim=obs_dim, output_dim=n_actions)\n",
    "target_network = Qnet(input_dim=obs_dim, output_dim=n_actions)\n",
    "replay = ExperienceReplay(observation_dim=obs_dim, size=2000, batch_size=32)\n",
    "\n",
    "# Make a learner:\n",
    "learner = DQN(\n",
    "    main_network,\n",
    "    target_network,\n",
    "    replay,\n",
    "    gamma=0.90,\n",
    "    train_interval=1,\n",
    "    copy_interval=100,\n",
    "    epsilon=lambda i: 0.5\n",
    ")\n",
    "num_episodes = 250\n",
    "returns, lengths = learner.learn(\n",
    "    env=env,\n",
    "    num_episodes=num_episodes,\n",
    ")\n",
    "\n",
    "return_value = learner.rollout(env, episode=0, train=False, apply_epsilon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlmep.utils.plot_returns import plot_returns\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "plot_returns(ax[0], returns, window=25)\n",
    "\n",
    "ax[1].plot(env.history)\n",
    "ax[1].set_xlabel(\"Step\")\n",
    "ax[1].set_ylabel(\"Energy\")\n",
    "ax[1].set_title(f\"Return: {return_value:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlmep.exercise_2 import plot_qvalues\n",
    "\n",
    "# Calculate the Q-values for each state in the grid:\n",
    "Q_table_nn = np.zeros((env.gridspec.grid_size[0], env.gridspec.grid_size[1], 4))\n",
    "for i in range(env.gridspec.grid_size[0]):\n",
    "    for j in range(env.gridspec.grid_size[1]):\n",
    "        Q_table_nn[i, j, :] = (\n",
    "            main_network(torch.tensor([i, j], dtype=torch.float))\n",
    "            .detach()\n",
    "            .numpy()\n",
    "            .flatten()\n",
    "        )\n",
    "\n",
    "# Plot the Q-values:\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "env.reset()\n",
    "env.visualize(ax=ax, dx=0.5, dy=0.5, plot_moving=False)\n",
    "ax = plot_qvalues(ax, Q_table_nn, env, alpha=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlmep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
