{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlmep.envs import ReactorEnv\n",
    "from rlmep.agents.reinforce import ReinforceAgent, PolicyNetwork\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HalfNormal.__init__() got multiple values for argument 'validate_args'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m mu = torch.tensor([\u001b[32m0.1\u001b[39m, \u001b[32m2.0\u001b[39m])\n\u001b[32m      4\u001b[39m sigma = torch.tensor([\u001b[32m0.1\u001b[39m, \u001b[32m2.0\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m distribution = \u001b[43mHalfNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m action = distribution.sample()\n\u001b[32m     10\u001b[39m logp = distribution.log_prob(action)\n",
      "\u001b[31mTypeError\u001b[39m: HalfNormal.__init__() got multiple values for argument 'validate_args'"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Normal, HalfNormal\n",
    "\n",
    "mu = torch.tensor([0.1, 2.0])\n",
    "sigma = torch.tensor([0.1, 2.0])\n",
    "\n",
    "distribution = HalfNormal(mu, sigma, validate_args=False)\n",
    "\n",
    "action = distribution.sample()\n",
    "\n",
    "logp = distribution.log_prob(action)\n",
    "\n",
    "logp = logp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ReactorEnv()\n",
    "policy = PolicyNetwork(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m action, logp = \u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAction:\u001b[39m\u001b[33m\"\u001b[39m, action)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLog probability:\u001b[39m\u001b[33m\"\u001b[39m, logp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:6\u001b[39m, in \u001b[36msample_action\u001b[39m\u001b[34m(self, x)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/rlmep/.venv/lib/python3.11/site-packages/torch/distributions/half_normal.py:39\u001b[39m, in \u001b[36mHalfNormal.__init__\u001b[39m\u001b[34m(self, scale, validate_args)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, scale, validate_args=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     38\u001b[39m     base_dist = Normal(\u001b[32m0\u001b[39m, scale, validate_args=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbase_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAbsTransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/rlmep/.venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:102\u001b[39m, in \u001b[36mTransformedDistribution.__init__\u001b[39m\u001b[34m(self, base_distribution, transforms, validate_args)\u001b[39m\n\u001b[32m    100\u001b[39m batch_shape = forward_shape[:cut]\n\u001b[32m    101\u001b[39m event_shape = forward_shape[cut:]\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/rlmep/.venv/lib/python3.11/site-packages/torch/distributions/distribution.py:52\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validate_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_args = validate_args\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._validate_args:\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     54\u001b[39m         arg_constraints = \u001b[38;5;28mself\u001b[39m.arg_constraints\n",
      "\u001b[31mRuntimeError\u001b[39m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "action, logp = policy.sample_action(torch.tensor([0.0, 0.0, 0.0, 0.0], dtype=torch.float32))\n",
    "\n",
    "print(\"Action:\", action)\n",
    "print(\"Log probability:\", logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m agent = ReinforceAgent(policy, env)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplay_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/rlmep/src/rlmep/agents/reinforce.py:75\u001b[39m, in \u001b[36mReinforceAgent.play_episode\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     72\u001b[39m action, log_probs = \u001b[38;5;28mself\u001b[39m.policy.sample_action(torch.tensor(state, dtype=torch.float32))\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Take a step in the environment with the action:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m next_state, reward, terminal, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Append the step to the trajectory:\u001b[39;00m\n\u001b[32m     78\u001b[39m trajectory.append(state, action, reward, log_probs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/rlmep/src/rlmep/envs/reactor.py:46\u001b[39m, in \u001b[36mReactorEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]):\n\u001b[32m     38\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m    Perform a step in the environment.\u001b[39;00m\n\u001b[32m     40\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m \u001b[33;03m        Feed rates for A and B in mol/s.\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     feed_A, feed_B = action\n\u001b[32m     47\u001b[39m     feed_A = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(feed_A, \u001b[32m0.0\u001b[39m), \u001b[32m1.0\u001b[39m)  \u001b[38;5;66;03m# Ensure non-negative feed rates\u001b[39;00m\n\u001b[32m     48\u001b[39m     feed_B = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(feed_B, \u001b[32m0.0\u001b[39m), \u001b[32m1.0\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "agent = ReinforceAgent(policy, env)\n",
    "agent.play_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "\n",
    "state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "action, log_p = policy.sample_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlmep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
