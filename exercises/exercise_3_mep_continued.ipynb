{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import rlmep\n",
    "\n",
    "    print(\"Already installed\")\n",
    "except ImportError:\n",
    "    %pip install \"rlmep @ git+https://github.com/Mads-PeterVC/rlmep.git\" # if you are using Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from ase.calculators.emt import EMT\n",
    "from ase.visualize import view\n",
    "\n",
    "from rlmep.data import get_cluster_data\n",
    "from rlmep.exercise_1 import DQN, ExperienceReplay\n",
    "from rlmep.exercise_2 import GridSpec, plot_qvalues\n",
    "from rlmep.exercise_2.discrete_mep_scaffold import (\n",
    "    ScaffoldDiscreteMEP,\n",
    "    _get_cheat_functions,\n",
    ")\n",
    "from rlmep.exercise_3 import plot_levels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Slightly) More interesting example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've created a slightly more interesting problem where the atom does not move in a straight line,\n",
    "but has to move around (a few) other atoms in two-dimensional space.\n",
    "\n",
    "I've created three 'levels' for this problem with the atom starting in different positions,\n",
    "set by the `level` parameter (`0`, `1`, or `2`).\n",
    "\n",
    "Below, the initial and final configurations for the three levels are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_levels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's load the most simple case, level 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_config, final_config = get_cluster_data(level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to setup the environment which involves a few steps; \n",
    "\n",
    "Setting up the grid: \n",
    "- Choose the `grid_size`\n",
    "- Choose a `grid_spacing`\n",
    "- Choose the lower-left `corner` of the grid in (x, y)-coordinates.\n",
    "- Choose the z-coordinate of the moving atom.\n",
    "\n",
    "Hyperparameters; \n",
    "- `barrier_max`: The barrier that corresponds to a reward of 0 ($\\Delta_{\\mathrm{max}}$ in the reward function).\n",
    "- `reward_scale`: Corresponds to $A$ in the reward function.\n",
    "\n",
    "Others; \n",
    "- `moving_atom`: is the index of the atom being moved - in this case the last atom.\n",
    "\n",
    "#### Exercise: \n",
    "\n",
    "Set up the environment, do some sanity checking and then try having the DQN algorithm \n",
    "solve the problem.\n",
    "\n",
    "Once you've solved level 0, try one of the others. \n",
    "\n",
    "<details>\n",
    "<summary> <strong> Hint: Grid level 0 </strong> </summary>\n",
    "\n",
    "For level 0 this grid is solvable for me.\n",
    "\n",
    "```python\n",
    "grid_spec = GridSpec(\n",
    "    grid_size=(15, 8),\n",
    "    grid_spacing=0.5,\n",
    "    corner=(10, 14),\n",
    "    height=initial_config.positions[0][2],\n",
    "```\n",
    "</details>\n",
    "<br> \n",
    "\n",
    "<details>\n",
    "<summary> <strong> Hint: Settings level 0 </strong> </summary>\n",
    "\n",
    "I've found \n",
    "```python\n",
    "barrier_max = 2.5\n",
    "reward_scale = 10.0\n",
    "distance_parameter = 0.1\n",
    "```\n",
    "to work.\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My versions of the functions from exercise 2 (`check_terminal`, `check_truncated`, etc.)\n",
    "functions = _get_cheat_functions()\n",
    "\n",
    "initial_config, final_config = get_cluster_data(level=0)\n",
    "\n",
    "grid_spec = ...\n",
    "\n",
    "env = ScaffoldDiscreteMEP(\n",
    "    initial_config=initial_config,\n",
    "    final_config=final_config,\n",
    "    gridspec=grid_spec,\n",
    "    functions=functions,\n",
    "    moving_atom=-1,\n",
    "    max_steps=100,\n",
    "    barrier_max=..., # Your decision here\n",
    "    reward_scale=..., # Your decision here\n",
    "    distance_parameter=..., # Your decision here\n",
    "    calculator=EMT(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a rollout to visualize a path in the environment\n",
    "state, _ = env.reset()\n",
    "\n",
    "x_steps = 9\n",
    "y_steps = 4\n",
    "action_sequence = [2] * y_steps + [1] * x_steps + [3] * y_steps  # up → right → down\n",
    "state_history = [state]\n",
    "return_value = 0\n",
    "for action in action_sequence:\n",
    "    state, reward, terminal, truncated, info = env.step(action)\n",
    "    state_history.append(state)\n",
    "    return_value += reward\n",
    "    if terminal or truncated:\n",
    "        break\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(2 * 4, 4))\n",
    "\n",
    "env.reset()\n",
    "env.visualize(ax=ax[0], dx=0.5, dy=2, plot_moving=False, state_history=state_history)\n",
    "\n",
    "delta_path = info[\"history\"]\n",
    "max_barrier = np.max(delta_path)\n",
    "index_max = np.argmax(delta_path)\n",
    "\n",
    "ax[1].plot(delta_path, \"-o\", label=\"Energy\")\n",
    "ax[1].plot([index_max, index_max], [0, max_barrier], \"-ro\", label=f\"Max Barrier: {max}\")\n",
    "ax[1].set_xlabel(\"Step\")\n",
    "ax[1].set_title(f\"Return: {return_value:0.2f} \\n Barrier: {max_barrier:0.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can again either use my implementation or plug in your own for `Qnet` and `DQN` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlmep.exercise_1.dqn_learner import DQN\n",
    "from rlmep.exercise_1.qnet import Qnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# Set the action and observation dimensions:\n",
    "obs_dim = 2\n",
    "n_actions = 4\n",
    "\n",
    "# Set the network settings:\n",
    "main_network = ...\n",
    "target_network = ...\n",
    "replay = ...\n",
    "\n",
    "# Make a learner:\n",
    "learner = ...\n",
    "num_episodes = ...\n",
    "\n",
    "returns, lengths = learner.learn(\n",
    "    env=env,\n",
    "    num_episodes=num_episodes,\n",
    ")\n",
    "\n",
    "return_value, states = learner.rollout(env, episode=0, train=False, apply_epsilon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlmep.utils.plot_returns import plot_returns\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "plot_returns(ax[0], returns, window=25)\n",
    "\n",
    "delta_path = env.history\n",
    "max_barrier = np.max(delta_path)\n",
    "index_max = np.argmax(delta_path)\n",
    "ax[1].plot(delta_path, \"-o\", label=\"Energy\")\n",
    "ax[1].plot([index_max, index_max], [0, max_barrier], \"-ro\")\n",
    "ax[1].set_xlabel(\"Step\")\n",
    "ax[1].set_title(f\"Return: {return_value:0.2f} \\n Barrier: {max_barrier:0.2f}\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "Q_table_nn = np.zeros((env.gridspec.grid_size[0], env.gridspec.grid_size[1], 4))\n",
    "for i in range(env.gridspec.grid_size[0]):\n",
    "    for j in range(env.gridspec.grid_size[1]):\n",
    "        Q_table_nn[i, j, :] = (\n",
    "            main_network(torch.tensor([i, j], dtype=torch.float))\n",
    "            .detach()\n",
    "            .numpy()\n",
    "            .flatten()\n",
    "        )\n",
    "\n",
    "env.reset()\n",
    "env.visualize(ax=ax, dx=0.5, dy=2, plot_moving=False, state_history=states)\n",
    "# ax = plot_qvalues(ax, Q_table_nn, env, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain knowledge\n",
    "\n",
    "Obviously this agent is not the strongest - one problem in particular is that it \n",
    "doesn't *transfer* at all. Training on this problem does not help for another problem -\n",
    "at least not in general. \n",
    "\n",
    "This can most likely be improved by incorporating techniques used in modern MLIPs.\n",
    "Specifically the reason that there is no transfer is that the networks use a state \n",
    "representation that does not generalize at all. \n",
    "\n",
    "So a few ideas to improve on this are; \n",
    "\n",
    "- Use an invariant feature of the moving atom to predict actions\n",
    "    - I think this would already be helpful.\n",
    "    - However, I suspect that problems can arise from states that have the same invariant features, but different actions should be taken. \n",
    "- Rather than using an invariant feature of the state to predict all of the Q-values for each action, use invariant features of the state that an action would lead to. \n",
    "    - For our setup with 4 actions this means e.g. moving the atom to each adjacent grid-point and calculating SOAP features. \n",
    "    - Feed each of these four SOAP vectors through a network to predict $Q(s, a)$. \n",
    "- Use equivariant vector features\n",
    "    - E.g. for the moving predict a vector, like the force, and use that vector to predict Q-values.\n",
    "    - E.g. the dot-product between the vector and the grid-direction, so the Q-value for up would be $Q(s, \\uparrow) = \\vec{v}_{\\theta}(s) \\cdot \\uparrow$ where $\\vec{v}_{\\theta}(s)$ is the predicted equivariant vector. \n",
    "- Finally equivariance would also allow the problem to be restated as an environment with both a continuous action and state space. \n",
    "    - Requires using different RL algorithms - probably a policy gradient method.    \n",
    "\n",
    "Of course the environment can, and to be useful should, also be extended to moving more \n",
    "atoms simultaneously. One idea to do so; \n",
    "\n",
    "- Reformulate the action space to be the acceleration-vector on each atom. \n",
    "    - Have an MD-type driver propagate the system based on the RL-predicted acceleration vector.\n",
    "\n",
    "#### The exercise that makes sure you won't finish everything\n",
    "\n",
    "If you've come this far and want another exercise try to implement a Q-network that uses invariant features of the moving atom. \n",
    "\n",
    "To get started on that I suggest doing the following; \n",
    "\n",
    "1. Decide on an atomic feature, e.g. SOAP/symmetry functions - or the output of an invariant neural network. \n",
    "2. Adapt our `Qnet` to take this input (It basically can already, just need to adjust the input dimension)\n",
    "3. Train this network towards a Q-map from one of the current agents or some handcrafted ones.\n",
    "4. Train the network on half the Q-values of the current network and see if it generalizes. \n",
    "5. Try DQN with that network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
